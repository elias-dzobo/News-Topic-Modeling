{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd02db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37",
   "display_name": "Python 3.8.3 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "data = pd.read_csv('Eluvio_DS_Challenge.csv')\n",
    "\n",
    "data_text = data['title']\n",
    "\n",
    "documents = data_text\n",
    "\n",
    "n = len(documents)\n",
    "train_n = int(n*0.8)\n",
    "\n",
    "train = documents[:train_n]\n",
    "validate = documents[train_n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "509236\n407388\n101848\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))\n",
    "print(len(train))\n",
    "print(len(validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0                  Scores killed in Pakistan clashes\n",
       "1                   Japan resumes refuelling mission\n",
       "2                    US presses Egypt on Gaza border\n",
       "3       Jump-start economy: Give health care to all \n",
       "4    Council of Europe bashes EU&UN terror blacklist\n",
       "Name: title, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Elias\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "go\n"
     ]
    }
   ],
   "source": [
    "print(WordNetLemmatizer().lemmatize('went', pos = 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   original word stemmed\n",
       "0       caresses  caress\n",
       "1          flies     fli\n",
       "2           dies     die\n",
       "3          mules    mule\n",
       "4         denied    deni\n",
       "5           died     die\n",
       "6         agreed    agre\n",
       "7          owned     own\n",
       "8        humbled   humbl\n",
       "9          sized    size\n",
       "10       meeting    meet\n",
       "11       stating   state\n",
       "12       siezing    siez\n",
       "13   itemization    item\n",
       "14   sensational  sensat\n",
       "15   traditional  tradit\n",
       "16     reference   refer\n",
       "17     colonizer   colon\n",
       "18       plotted    plot"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>original word</th>\n      <th>stemmed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>caresses</td>\n      <td>caress</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>flies</td>\n      <td>fli</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>dies</td>\n      <td>die</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>mules</td>\n      <td>mule</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>denied</td>\n      <td>deni</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>died</td>\n      <td>die</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>agreed</td>\n      <td>agre</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>owned</td>\n      <td>own</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>humbled</td>\n      <td>humbl</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>sized</td>\n      <td>size</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>meeting</td>\n      <td>meet</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>stating</td>\n      <td>state</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>siezing</td>\n      <td>siez</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>itemization</td>\n      <td>item</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>sensational</td>\n      <td>sensat</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>traditional</td>\n      <td>tradit</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>reference</td>\n      <td>refer</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>colonizer</td>\n      <td>colon</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>plotted</td>\n      <td>plot</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied', 'died', 'agreed', 'owned',\n",
    "'humbled', 'sized', 'meeting', 'stating', 'siezing', 'itemization', 'sensational', 'traditional', 'reference', 'colonizer', 'plotted']\n",
    "\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "\n",
    "pd.DataFrame(data = {'original word': original_words, 'stemmed': singles})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for preprocessing\n",
    "\n",
    "def lemmatize_stem(text):\n",
    "\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos = 'v'))\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "\n",
    "            result.append(lemmatize_stem(token))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[NOTICE} riginal document..\n['Scores', 'killed', 'in', 'Pakistan', 'clashes']\n\n\nTokemized and Lemmatized document: \n['score', 'kill', 'pakistan', 'clash']\n"
     ]
    }
   ],
   "source": [
    "doc_num = 3000\n",
    "\n",
    "doc_sample = documents[:doc_num].values[0]\n",
    "\n",
    "print(\"[NOTICE} riginal document..\")\n",
    "\n",
    "words = []\n",
    "\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "\n",
    "print(words)\n",
    "print('\\n\\nTokemized and Lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(train)\n",
    "validate = pd.DataFrame(validate)\n",
    "\n",
    "processed_doc = train['title'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0                       [score, kill, pakistan, clash]\n",
       "1                      [japan, resum, refuel, mission]\n",
       "2                         [press, egypt, gaza, border]\n",
       "3                 [jump, start, economi, health, care]\n",
       "4            [council, europ, bash, terror, blacklist]\n",
       "5    [presto, farmer, unveil, illeg, mock, tudor, c...\n",
       "6    [strike, protest, gridlock, poland, ukrain, bo...\n",
       "7                                  [mismanag, program]\n",
       "8                 [nicola, sarkozi, threaten, ryanair]\n",
       "9    [plan, missil, shield, polish, town, resist, v...\n",
       "Name: title, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "processed_doc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 clash\n1 kill\n2 pakistan\n3 score\n4 japan\n5 mission\n6 refuel\n7 resum\n8 border\n9 egypt\n10 gaza\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for k, v in dictionary.iteritems():\n",
    "\n",
    "    print(k, v)\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    if count > 10:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_doc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(54, 1), (254, 1), (2238, 1), (2921, 1), (4903, 1)]"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "bow_corpus[doc_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "word 54 (' time ') appears 1 times\nword 254 (' china ') appears 1 times\nword 2238 (' tibet ') appears 1 times\nword 2921 (' seal ') appears 1 times\nword 4903 (' gateway ') appears 1 times\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[doc_num]\n",
    "\n",
    "for i in range(len(bow_doc_4310)):\n",
    "\n",
    "    print('word {} (\\' {} \\') appears {} times'.format(bow_doc_4310[i][0], dictionary[bow_doc_4310[i][0]], bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TfidfModel(num_docs=407388, num_nnz=3406602)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(4, 0.3015512525870183), (5, 0.4164584373227593), (6, 0.7092460874421703), (7, 0.4822853921630416)]\n"
     ]
    }
   ],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "print(corpus_tfidf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(0, 0.5083535236881371),\n (1, 0.2993426062832652),\n (2, 0.42515432580274004),\n (3, 0.6864506524642643)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "\n",
    "    pprint(doc)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(\n",
    "    bow_corpus,\n",
    "    num_topics= 2,\n",
    "    id2word= dictionary,\n",
    "    passes= 2,\n",
    "    workers = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Topics 0 \nWords: 0.014*\"say\" + 0.010*\"russia\" + 0.009*\"china\" + 0.009*\"syria\" + 0.008*\"kill\" + 0.007*\"isi\" + 0.007*\"russian\" + 0.007*\"state\" + 0.007*\"iran\" + 0.006*\"ukrain\" \n\n\nTopics 1 \nWords: 0.008*\"polic\" + 0.006*\"year\" + 0.006*\"protest\" + 0.005*\"say\" + 0.005*\"isra\" + 0.005*\"death\" + 0.005*\"right\" + 0.005*\"palestinian\" + 0.005*\"kill\" + 0.005*\"arrest\" \n\n\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "\n",
    "    print('Topics {} \\nWords: {} '.format(idx, topic))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['china', 'seal', 'gateway', 'tibet', 'time']"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "processed_doc[4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nScore 0.6808820366859436\t \nTopics: 0.014*\"say\" + 0.010*\"russia\" + 0.009*\"china\" + 0.009*\"syria\" + 0.008*\"kill\" + 0.007*\"isi\" + 0.007*\"russian\" + 0.007*\"state\" + 0.007*\"iran\" + 0.006*\"ukrain\"\n\nScore 0.3191179633140564\t \nTopics: 0.008*\"polic\" + 0.006*\"year\" + 0.006*\"protest\" + 0.005*\"say\" + 0.005*\"isra\" + 0.005*\"death\" + 0.005*\"right\" + 0.005*\"palestinian\" + 0.005*\"kill\" + 0.005*\"arrest\"\n"
     ]
    }
   ],
   "source": [
    "doc_num = 4000\n",
    "for index, score in sorted(lda_model[bow_corpus[doc_num]], key = lambda tup: -1*tup[1]):\n",
    "\n",
    "    print('\\nScore {}\\t \\nTopics: {}'.format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Score: 0.5964456796646118\t Topic: 0.014*\"say\" + 0.010*\"russia\" + 0.009*\"china\" + 0.009*\"syria\" + 0.008*\"kill\" + 0.007*\"isi\" + 0.007*\"russian\" + 0.007*\"state\" + 0.007*\"iran\" + 0.006*\"ukrain\"\nScore: 0.40355435013771057\t Topic: 0.008*\"polic\" + 0.006*\"year\" + 0.006*\"protest\" + 0.005*\"say\" + 0.005*\"isra\" + 0.005*\"death\" + 0.005*\"right\" + 0.005*\"palestinian\" + 0.005*\"kill\" + 0.005*\"arrest\"\n"
     ]
    }
   ],
   "source": [
    "unseen = validate['title'].tolist()[0]\n",
    "\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "\n",
    "    print('Score: {}\\t Topic: {}'.format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Score: 0.6998955607414246\t Topic: 0.014*\"say\" + 0.010*\"russia\" + 0.009*\"china\" + 0.009*\"syria\" + 0.008*\"kill\"\nScore: 0.30010440945625305\t Topic: 0.008*\"polic\" + 0.006*\"year\" + 0.006*\"protest\" + 0.005*\"say\" + 0.005*\"isra\"\n"
     ]
    }
   ],
   "source": [
    "check = validate['title'].tolist()[5]\n",
    "\n",
    "check_vec = dictionary.doc2bow(preprocess(check))\n",
    "\n",
    "for index, score in sorted(lda_model[check_vec], key=lambda tup: -1*tup[1]):\n",
    "\n",
    "    print('Score: {}\\t Topic: {}'.format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}